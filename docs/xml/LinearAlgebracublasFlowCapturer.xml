<?xml version='1.0' encoding='UTF-8' standalone='no'?>
<doxygen xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:noNamespaceSchemaLocation="compound.xsd" version="1.8.14">
  <compounddef id="LinearAlgebracublasFlowCapturer" kind="page">
    <compoundname>LinearAlgebracublasFlowCapturer</compoundname>
    <title>Linear Algebra with cuBLAS</title>
    <tableofcontents/>
    <briefdescription>
    </briefdescription>
    <detaileddescription>
<para>Taskflow provides a library tf::cublasFlowCapturer to program and accelerate <emphasis>basic linear algebra subprograms</emphasis> (BLAS) on top of <ulink url="https://docs.nvidia.com/cuda/cublas/index.html">cuBLAS</ulink>.</para><para><image type="html" name="LinearAlgebra.png" width="50%"></image>
</para><sect1 id="LinearAlgebracublasFlowCapturer_1WhatIsBLAS">
<title>What is BLAS?</title>
<para>The BLAS (Basic Linear Algebra Subprograms) are routines that provide standard building blocks for performing basic vector and matrix operations. There are three levels:</para><para><orderedlist>
<listitem><para>Level 1: performs scalar, vector, and vector-vector operations</para></listitem><listitem><para>Level 2: performs matrix-vector operations</para></listitem><listitem><para>Level 3: performs matrix-matrix operations</para></listitem></orderedlist>
</para><para>BLAS is commonly used by linear algebra software. The <ulink url="https://docs.nvidia.com/cuda/cublas/index.html">cuBLAS</ulink> library is an implementation of BLAS (Basic Linear Algebra Subprograms) on top of the Nvidia CUDA runtime and it allows users to access the computational resources of Nvidia GPUs.</para></sect1>
<sect1 id="LinearAlgebracublasFlowCapturer_1HowToUsecublasFlowCapturer">
<title>What is a cublasFlow Capturer? Why?</title>
<para>tf::cublasFlowCapturer provides an interface over native cuBLAS functions and allows users to express linear algebra algorithms using a <emphasis>task graph model</emphasis>. We transform the task graph into a CUDA graph using a stream capture algorithm optimized for maximum concurrency. When a cuBLAS program is transformed into a CUDA graph, we can launch the entire graph using a single kernel call. This organization minimizes kernels launch overhead and allows the CUDA runtime to optimize the whole workflow. The following example (<computeroutput>cublasflow.cu</computeroutput>) use tf::cublasFlowCapturer to perform a 2-norm operation on a vector.</para><para><programlisting filename=".cpp"><codeline><highlight class="preprocessor">#include<sp/>&lt;taskflow/cublasflow.hpp&gt;</highlight><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"></highlight><highlight class="keywordtype">int</highlight><highlight class="normal"><sp/>main()<sp/>{</highlight></codeline>
<codeline><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/></highlight><highlight class="keyword">const</highlight><highlight class="normal"><sp/></highlight><highlight class="keywordtype">int</highlight><highlight class="normal"><sp/>N<sp/>=<sp/>1024;</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><ref refid="classtf_1_1Executor" kindref="compound">tf::Executor</ref><sp/>executor;</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><ref refid="classtf_1_1Taskflow" kindref="compound">tf::Taskflow</ref><sp/>taskflow(</highlight><highlight class="stringliteral">&quot;cublas<sp/>2-norm&quot;</highlight><highlight class="normal">);<sp/><sp/></highlight><highlight class="comment">//<sp/>initialize<sp/>an<sp/>unit<sp/>vector</highlight><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><ref refid="cpp/container/vector" kindref="compound" external="/home/twhuang/Code/taskflow/doxygen/cppreference-doxygen-web.tag.xml">std::vector&lt;float&gt;</ref><sp/>hvec(N,<sp/>1);<sp/></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordtype">float</highlight><highlight class="normal"><sp/><sp/>hres;<sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/></highlight><highlight class="comment">//<sp/>cpu<sp/>vector</highlight><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordtype">float</highlight><highlight class="normal">*<sp/>gvec<sp/>=<sp/>tf::cuda_malloc_device&lt;float&gt;(N);<sp/><sp/></highlight><highlight class="comment">//<sp/>gpu<sp/>vector</highlight><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordtype">float</highlight><highlight class="normal">*<sp/>gres<sp/>=<sp/>tf::cuda_malloc_device&lt;float&gt;(1);<sp/><sp/></highlight><highlight class="comment">//<sp/>gpu<sp/>result</highlight><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/>taskflow.<ref refid="classtf_1_1FlowBuilder_1a60d7a666cab71ecfa3010b2efb0d6b57" kindref="member">emplace</ref>([&amp;](<ref refid="classtf_1_1cudaFlowCapturer" kindref="compound">tf::cudaFlowCapturer</ref>&amp;<sp/>capturer){</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="comment">//<sp/>create<sp/>a<sp/>cuBLAS<sp/>flow<sp/>capturer</highlight><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keyword">auto</highlight><highlight class="normal"><sp/>blas<sp/>=<sp/>capturer.make_capturer&lt;tf::cublasFlowCapturer&gt;();</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/><ref refid="classtf_1_1cudaTask" kindref="compound">tf::cudaTask</ref><sp/>h2d<sp/>=<sp/>capturer.<ref refid="classtf_1_1cudaFlowCapturer_1ab70f12050e78b588f5c23d874aa4e538" kindref="member">copy</ref>(gvec,<sp/>hvec.data(),<sp/>N).name(</highlight><highlight class="stringliteral">&quot;h2d&quot;</highlight><highlight class="normal">);</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/><ref refid="classtf_1_1cudaTask" kindref="compound">tf::cudaTask</ref><sp/>nrm<sp/>=<sp/>blas-&gt;nrm2(N,<sp/>gvec,<sp/>1,<sp/>gres).<ref refid="classtf_1_1cudaTask_1ab81b4f71a44af8d61758524f0c274962" kindref="member">name</ref>(</highlight><highlight class="stringliteral">&quot;2-norm&quot;</highlight><highlight class="normal">);</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/><ref refid="classtf_1_1cudaTask" kindref="compound">tf::cudaTask</ref><sp/>d2h<sp/>=<sp/>capturer.<ref refid="classtf_1_1cudaFlowCapturer_1ab70f12050e78b588f5c23d874aa4e538" kindref="member">copy</ref>(&amp;hres,<sp/>gres,<sp/>1).<ref refid="classtf_1_1cudaTask_1ab81b4f71a44af8d61758524f0c274962" kindref="member">name</ref>(</highlight><highlight class="stringliteral">&quot;d2h&quot;</highlight><highlight class="normal">);</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/>nrm.<ref refid="classtf_1_1cudaTask_1abdd68287ec4dff4216af34d1db44d1b4" kindref="member">precede</ref>(d2h)</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/>.<ref refid="classtf_1_1cudaTask_1a4a9ca1a34bac47e4c9b04eb4fb2f7775" kindref="member">succeed</ref>(h2d);</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/>}).name(</highlight><highlight class="stringliteral">&quot;capturer&quot;</highlight><highlight class="normal">);</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/>executor.<ref refid="classtf_1_1Executor_1a519777f5783981d534e9e53b99712069" kindref="member">run</ref>(taskflow).wait();</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/>taskflow.<ref refid="classtf_1_1Taskflow_1ac433018262e44b12c4cc9f0c4748d758" kindref="member">dump</ref>(<ref refid="cpp/io/basic_ostream" kindref="compound" external="/home/twhuang/Code/taskflow/doxygen/cppreference-doxygen-web.tag.xml">std::cout</ref>);</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/>assert(hres<sp/>==<sp/>32);</highlight></codeline>
<codeline><highlight class="normal">}</highlight></codeline>
</programlisting></para><para><dotfile name="/home/twhuang/Code/taskflow/doxygen/images/cublas_flow_capturer_norm2.dot"></dotfile>
</para><para>You need to link the <computeroutput>cublas</computeroutput> library when compiling a cublasFlow capturer program:</para><para><programlisting filename=".shell-session"><codeline><highlight class="normal">~$<sp/>nvcc<sp/>cublasflow.cpp<sp/>-I<sp/>path/to/taskflow/include<sp/>-lcublas</highlight></codeline>
</programlisting></para><para>Please refer to the page <ref refid="CompileTaskflowWithCUDA" kindref="compound">Compile Taskflow with CUDA</ref> for more details about compiling Taskflow with CUDA.</para></sect1>
<sect1 id="LinearAlgebracublasFlowCapturer_1DataModelOscublasFlowCapturer">
<title>Understand the Data Model</title>
<para>The data pointers used within tf::cublasFlowCapturer must sit in GPU memory space, including scalar pointers (<computeroutput>alpha</computeroutput> and <computeroutput>beta</computeroutput>), input pointers (e.g., vectors, matrices), and output pointers (e.g., result). By default, we set the pointer mode to <computeroutput>CUBLAS_POINTER_MODE_DEVICE</computeroutput>. You must allocate required matrices and vectors in the GPU memory space, fill them with data, call the methods defined in tf::cublasFlowCapturer, and then upload the results from GPU memory space back to the host.</para><para><simplesect kind="note"><para>tf::cublasFlowCapturer currently supports only <computeroutput>float</computeroutput> and <computeroutput>double</computeroutput> data types.</para></simplesect>
The cuBLAS library uses <emphasis>column-major storage</emphasis> and 1-based indexing. Since C/C++ adopts row-major layout, we cannot use the native array semantics when matrix-matrix or matrix-vector operations are involved. We often need extra transposition on input matrices before these operations can take place correctly. In terms of storage, a row-major matrix is equivalent to a transposed column-major matrix, as shown below:</para><para><formula id="0">\[ A_{RowMajor} \iff A^T_{ColumnMajor} \]</formula></para><sect2 id="LinearAlgebracublasFlowCapturer_1cublasFlowCapturerDataModelExample">
<title>Example: Transform Data Layout in Matrix Multiplication</title>
<para>Suppose we have a method <computeroutput>matmul(A, B, C)</computeroutput> that multiplies two matrices <computeroutput>A</computeroutput> and <computeroutput>B</computeroutput> and stores the result in <computeroutput>C</computeroutput>, using column-major storage. In C/C++, data layout is mostly row-major. Since we know a row-major matrix is equivalent in storage to a transposed column-major matrix, we can take a transposed view of this multiplication:</para><para><formula id="1">\[ C^T = B^T \times A^T \]</formula></para><para>If the given matrices <computeroutput>A</computeroutput>, <computeroutput>B</computeroutput>, and <computeroutput>C</computeroutput> are on row-major layout, calling <computeroutput>matmul(A, B, C)</computeroutput> is equivalent to the above transposed version. The function stores the result of transposed <computeroutput>C</computeroutput> in column-major storage which in turns translates to row-major layout of <computeroutput>C</computeroutput> <ndash/> <emphasis>our desired solution</emphasis>.</para></sect2>
</sect1>
<sect1 id="LinearAlgebracublasFlowCapturer_1cublasFlowCapturerLevel-1">
<title>Use Level-1 Methods</title>
<para>We currently support the following level-1 methods:</para><para><itemizedlist>
<listitem><para>tf::cublasFlowCapturer::amax finds the min element index of the max absolute magnitude in a vector</para></listitem><listitem><para>tf::cublasFlowCapturer::amin finds the min element index of the min magnitude in a vector</para></listitem><listitem><para>tf::cublasFlowCapturer::asum computes the sum of absolute values of elements over a vector</para></listitem><listitem><para>tf::cublasFlowCapturer::axpy multiplies a vector by a scalar and adds it to another vector</para></listitem><listitem><para>tf::cublasFlowCapturer::copy copies a vector into another vector</para></listitem><listitem><para>tf::cublasFlowCapturer::dot computes the dot product of two vectors</para></listitem><listitem><para>tf::cublasFlowCapturer::nrm2 computes the Euclidean norm of a vector</para></listitem><listitem><para>tf::cublasFlowCapturer::scal multiples a vector by a scalar</para></listitem><listitem><para>tf::cublasFlowCapturer::swap interchanges the elements of two vectors</para></listitem></itemizedlist>
</para><para>Our level-1 methods capture the native <ulink url="https://docs.nvidia.com/cuda/cublas/index.html#cublas-level-1-function-reference">cublas level-1 calls</ulink> with internal stream(s) optimized for maximum concurrency. The two scalars, <computeroutput>alpha</computeroutput> and <computeroutput>beta</computeroutput>, and input and output matrices must sit in GPU memory space.</para></sect1>
<sect1 id="LinearAlgebracublasFlowCapturer_1cublasFlowCapturerLeve2-1">
<title>Use Level-2 Methods</title>
<para>We currently support the following level-2 methods:</para><para><itemizedlist>
<listitem><para>tf::cublasFlowCapturer::gemv performs general matrix-vector multiplication</para></listitem><listitem><para>tf::cublasFlowCapturer::c_gemv performs general matrix-vector multiplication on row-major layout</para></listitem><listitem><para>tf::cublasFlowCapturer::symv performs symmetric matrix-vector multiplication</para></listitem><listitem><para>tf::cublasFlowCapturer::c_symv performs symmetric matrix-vector multiplication on row-major layout</para></listitem><listitem><para>tf::cublasFlowCapturer::syr performs symmetric rank-1 update</para></listitem><listitem><para>tf::cublasFlowCapturer::c_syr performs symmetric rank-1 update on row-major layout</para></listitem><listitem><para>tf::cublasFlowCapturer::syr2 performs symmetric rank-2 update</para></listitem><listitem><para>tf::cublasFlowCapturer::c_syr2 performs symmetric rank-2 update on row-major layout</para></listitem><listitem><para>tf::cublasFlowCapturer::trmv performs triangular matrix-vector multiplication</para></listitem><listitem><para>tf::cublasFlowCapturer::c_trmv performs triangular matrix-vector multiplication on row-major layout</para></listitem><listitem><para>tf::cublasFlowCapturer::trsv solves triangular linear system with a single right-hand-side</para></listitem><listitem><para>tf::cublasFlowCapturer::c_trsv solves triangular linear system with a single right-hand-side on row-major layout</para></listitem></itemizedlist>
</para><para>Our level-2 methods capture the native <ulink url="https://docs.nvidia.com/cuda/cublas/index.html#cublas-level-2-function-reference">cublas level-2 calls</ulink> with internal stream(s) optimized for maximum concurrency. The two scalars, <computeroutput>alpha</computeroutput> and <computeroutput>beta</computeroutput>, and input and output matrices must sit in GPU memory space.</para><sect2 id="LinearAlgebracublasFlowCapturer_1cublasFlowCapturerLevel2Example">
<title>Example: Solve a Triangular Linear System</title>
<para>The following program solves a triangular linear system on row-major layout using tf::cublasFlowCapturer::c_trsv:</para><para><programlisting filename=".cpp"><codeline><highlight class="preprocessor">#include<sp/>&lt;taskflow/cublasflow.hpp&gt;</highlight><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"></highlight><highlight class="keywordtype">int</highlight><highlight class="normal"><sp/>main()<sp/>{</highlight></codeline>
<codeline><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordtype">int</highlight><highlight class="normal"><sp/>N<sp/>=<sp/>3;</highlight></codeline>
<codeline><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/></highlight><highlight class="keyword">const</highlight><highlight class="normal"><sp/><ref refid="cpp/container/vector" kindref="compound" external="/home/twhuang/Code/taskflow/doxygen/cppreference-doxygen-web.tag.xml">std::vector&lt;float&gt;</ref><sp/>hA<sp/>=<sp/>{</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/>1,<sp/>0,<sp/>0,<sp/><sp/></highlight><highlight class="comment">//<sp/>x1</highlight><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/>1,<sp/>1,<sp/>0,<sp/><sp/></highlight><highlight class="comment">//<sp/>x1<sp/>+<sp/>x2</highlight><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/>1,<sp/>1,<sp/>1<sp/><sp/><sp/></highlight><highlight class="comment">//<sp/>x1<sp/>+<sp/>x2<sp/>+<sp/>x3</highlight><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/>};</highlight></codeline>
<codeline><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/></highlight><highlight class="keyword">const</highlight><highlight class="normal"><sp/><ref refid="cpp/container/vector" kindref="compound" external="/home/twhuang/Code/taskflow/doxygen/cppreference-doxygen-web.tag.xml">std::vector&lt;float&gt;</ref><sp/>hB<sp/>=<sp/>{</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/>5,<sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/></highlight><highlight class="comment">//<sp/>x1<sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>=<sp/>5</highlight><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/>4,<sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/></highlight><highlight class="comment">//<sp/>x1<sp/>+<sp/>x2<sp/><sp/><sp/><sp/><sp/><sp/>=<sp/>4</highlight><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/>7<sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/></highlight><highlight class="comment">//<sp/>x1<sp/>+<sp/>x2<sp/>+<sp/>x3<sp/>=<sp/>7</highlight><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/>};</highlight></codeline>
<codeline><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><ref refid="cpp/container/vector" kindref="compound" external="/home/twhuang/Code/taskflow/doxygen/cppreference-doxygen-web.tag.xml">std::vector&lt;float&gt;</ref><sp/>r(N,<sp/>0);</highlight></codeline>
<codeline><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><ref refid="classtf_1_1Taskflow" kindref="compound">tf::Taskflow</ref><sp/>taskflow(</highlight><highlight class="stringliteral">&quot;Ax<sp/>=<sp/>b&quot;</highlight><highlight class="normal">);</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><ref refid="classtf_1_1Executor" kindref="compound">tf::Executor</ref><sp/>executor;</highlight></codeline>
<codeline><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordtype">float</highlight><highlight class="normal">*<sp/>dA<sp/>=<sp/>tf::cuda_malloc_device&lt;float&gt;(hA.size());</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordtype">float</highlight><highlight class="normal">*<sp/>dB<sp/>=<sp/>tf::cuda_malloc_device&lt;float&gt;(hB.size());</highlight></codeline>
<codeline><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/>taskflow.<ref refid="classtf_1_1FlowBuilder_1a60d7a666cab71ecfa3010b2efb0d6b57" kindref="member">emplace</ref>([&amp;](<ref refid="classtf_1_1cudaFlowCapturer" kindref="compound">tf::cudaFlowCapturer</ref>&amp;<sp/>capturer){</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/><ref refid="classtf_1_1cudaTask" kindref="compound">tf::cudaTask</ref><sp/>blas<sp/>=<sp/>capturer.make_capturer&lt;tf::cublasFlowCapturer&gt;();</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/><ref refid="classtf_1_1cudaTask" kindref="compound">tf::cudaTask</ref><sp/>h2dA<sp/>=<sp/>blas-&gt;copy(dA,<sp/>hA.data(),<sp/>hA.size()).name(</highlight><highlight class="stringliteral">&quot;copy<sp/>A&quot;</highlight><highlight class="normal">);</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/><ref refid="classtf_1_1cudaTask" kindref="compound">tf::cudaTask</ref><sp/>h2dB<sp/>=<sp/>blas-&gt;copy(dB,<sp/>hB.data(),<sp/>hB.size()).name(</highlight><highlight class="stringliteral">&quot;copy<sp/>B&quot;</highlight><highlight class="normal">);</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/><ref refid="classtf_1_1cudaTask" kindref="compound">tf::cudaTask</ref><sp/>trsv<sp/>=<sp/>blas-&gt;c_trsv(</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/>CUBLAS_FILL_MODE_LOWER,<sp/>CUBLAS_OP_N,<sp/>CUBLAS_DIAG_UNIT,<sp/></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/>N,<sp/>dA,<sp/>N,<sp/>dB,<sp/>1</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/>).<ref refid="classtf_1_1cudaTask_1ab81b4f71a44af8d61758524f0c274962" kindref="member">name</ref>(</highlight><highlight class="stringliteral">&quot;trsv&quot;</highlight><highlight class="normal">);</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/><ref refid="classtf_1_1cudaTask" kindref="compound">tf::cudaTask</ref><sp/>d2h<sp/>=<sp/>blas-&gt;copy(r.data(),<sp/>dB,<sp/>r.size()).name(</highlight><highlight class="stringliteral">&quot;copy<sp/>result&quot;</highlight><highlight class="normal">);</highlight></codeline>
<codeline><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/>trsv.<ref refid="classtf_1_1cudaTask_1a4a9ca1a34bac47e4c9b04eb4fb2f7775" kindref="member">succeed</ref>(h2dA,<sp/>h2dB)</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>.<ref refid="classtf_1_1cudaTask_1abdd68287ec4dff4216af34d1db44d1b4" kindref="member">precede</ref>(d2h);</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/>}).name(</highlight><highlight class="stringliteral">&quot;cublasFlow&quot;</highlight><highlight class="normal">);</highlight></codeline>
<codeline><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/>executor.<ref refid="classtf_1_1Executor_1a519777f5783981d534e9e53b99712069" kindref="member">run</ref>(taskflow).wait();</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><ref refid="cpp/io/basic_ostream" kindref="compound" external="/home/twhuang/Code/taskflow/doxygen/cppreference-doxygen-web.tag.xml">std::cout</ref><sp/>&lt;&lt;<sp/></highlight><highlight class="stringliteral">&quot;solution<sp/>of<sp/>the<sp/>linear<sp/>system:<sp/>\n&quot;</highlight><highlight class="normal">;</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordflow">for</highlight><highlight class="normal">(</highlight><highlight class="keywordtype">size_t</highlight><highlight class="normal"><sp/>i=0;<sp/>i&lt;r.size();<sp/>++i)<sp/>{</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/><ref refid="cpp/io/basic_ostream" kindref="compound" external="/home/twhuang/Code/taskflow/doxygen/cppreference-doxygen-web.tag.xml">std::cout</ref><sp/>&lt;&lt;<sp/></highlight><highlight class="stringliteral">&quot;x&quot;</highlight><highlight class="normal"><sp/>&lt;&lt;<sp/>i<sp/>&lt;&lt;<sp/></highlight><highlight class="stringliteral">&quot;:<sp/>&quot;</highlight><highlight class="normal"><sp/>&lt;&lt;<sp/>r[i]<sp/>&lt;&lt;<sp/></highlight><highlight class="charliteral">&apos;\n&apos;</highlight><highlight class="normal">;</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/>}</highlight></codeline>
<codeline><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/>0;</highlight></codeline>
<codeline><highlight class="normal">}</highlight></codeline>
</programlisting></para><para>The program uses one cudaFlow task that spawns a capturer of (1) two copy tasks, <computeroutput>h2dA</computeroutput> and <computeroutput>h2dB</computeroutput>, to copy the triangular matrix <computeroutput>A</computeroutput> and the solution vector <computeroutput>b</computeroutput>, (2) one kernel task <computeroutput>trsv</computeroutput> to solve the triangular linear system storing the result in <computeroutput>dB</computeroutput>, and (3) one copy task <computeroutput>d2h</computeroutput> to copy the solution in <computeroutput>dB</computeroutput> to <computeroutput>r</computeroutput>.</para><para><dotfile name="/home/twhuang/Code/taskflow/doxygen/images/cublasflow_capturer_trsv.dot"></dotfile>
</para></sect2>
</sect1>
<sect1 id="LinearAlgebracublasFlowCapturer_1cublasFlowCapturerLevel-3">
<title>Use Level-3 Methods</title>
<para>We currently support the following level-3 methods:</para><para><itemizedlist>
<listitem><para>tf::cublasFlowCapturer::geam performs matrix-matrix addition/transposition</para></listitem><listitem><para>tf::cublasFlowCapturer::c_geam performs matrix-matrix addition/transposition on row-major layout</para></listitem><listitem><para>tf::cublasFlowCapturer::gemm performs general matrix-matrix multiplication</para></listitem><listitem><para>tf::cublasFlowCapturer::c_gemm performs general matrix-matrix multiplication on row-major layout</para></listitem><listitem><para>tf::cublasFlowCapturer::gemm_batched performs batched general matrix-matrix multiplication</para></listitem><listitem><para>tf::cublasFlowCapturer::c_gemm_batched performs batched general matrix-matrix multiplication on row-major layout</para></listitem><listitem><para>tf::cublasFlowCapturer::gemm_sbatched performs batched general matrix-matrix multiplication with strided memory access</para></listitem><listitem><para>tf::cublasFlowCapturer::c_gemm_sbatched performs batched general matrix-matrix multiplication with strided memory access on row-major layout</para></listitem><listitem><para>tf::cublasFlowCapturer::symm performs symmetric matrix-matrix multiplication</para></listitem><listitem><para>tf::cublasFlowCapturer::c_symm performs symmetric matrix-matrix multiplicaiton on row-major layout</para></listitem><listitem><para>tf::cublasFlowCapturer::syrk performs symmetric rank-k update</para></listitem><listitem><para>tf::cublasFlowCapturer::c_syrk performs symmetric rank-k update on row-major layout</para></listitem><listitem><para>tf::cublasFlowCapturer::syr2k performs symmetric rank-2k update</para></listitem><listitem><para>tf::cublasFlowCapturer::c_syr2k performs symmetric rank-2k update on row-major layout</para></listitem><listitem><para>tf::cublasFlowCapturer::syrkx performs a variantion of symmetric rank-k update</para></listitem><listitem><para>tf::cublasFlowCapturer::c_syrkx performs a variantion of symmetric rank-k update on row-major layout</para></listitem><listitem><para>tf::cublasFlowCapturer::trmm performs triangular matrix-matrix multiplication</para></listitem><listitem><para>tf::cublasFlowCapturer::c_trmm performs triangular matrix-matrix multiplication on row-major layout</para></listitem><listitem><para>tf::cublasFlowCapturer::trsm solves a triangular linear system with multiple right-hand-sides</para></listitem><listitem><para>tf::cublasFlowCapturer::c_trsm solves a triangular linear system with multiple right-hand-sides on row-major layout</para></listitem></itemizedlist>
</para><para>Our level-3 methods capture the native <ulink url="https://docs.nvidia.com/cuda/cublas/index.html#cublas-level-2-function-reference">cublas level-3 calls</ulink> and <ulink url="https://docs.nvidia.com/cuda/cublas/index.html#blas-like-extension">cublas-extension calls</ulink> with internal stream(s) optimized for maximum concurrency. The two scalars, <computeroutput>alpha</computeroutput> and <computeroutput>beta</computeroutput>, and input and output matrices must sit in GPU memory space.</para><sect2 id="LinearAlgebracublasFlowCapturer_1cublasFlowCapturerLevel3Example">
<title>Example: Perform General Matrix-Matrix Multiplication</title>
<para>The following program performs general matrix multiplication on row-major layout using tf::cublasFlowCapturer::c_gemm:</para><para><programlisting filename=".cpp"><codeline><highlight class="preprocessor">#include<sp/>&lt;taskflow/cublasflow.hpp&gt;</highlight><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"></highlight><highlight class="keywordtype">int</highlight><highlight class="normal"><sp/>main()<sp/>{</highlight></codeline>
<codeline><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/></highlight><highlight class="keyword">const</highlight><highlight class="normal"><sp/></highlight><highlight class="keywordtype">int</highlight><highlight class="normal"><sp/>M<sp/>=<sp/>2,<sp/>N<sp/>=<sp/>4,<sp/>K<sp/>=<sp/>3;</highlight></codeline>
<codeline><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/></highlight><highlight class="keyword">const</highlight><highlight class="normal"><sp/><ref refid="cpp/container/vector" kindref="compound" external="/home/twhuang/Code/taskflow/doxygen/cppreference-doxygen-web.tag.xml">std::vector&lt;float&gt;</ref><sp/>hA<sp/>=<sp/>{<sp/><sp/><sp/><sp/><sp/><sp/></highlight><highlight class="comment">//<sp/>M<sp/>x<sp/>K<sp/>matrix</highlight><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/>11,<sp/>12,<sp/>13,<sp/></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/>14,<sp/>15,<sp/>16</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/>};</highlight></codeline>
<codeline><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/></highlight><highlight class="keyword">const</highlight><highlight class="normal"><sp/><ref refid="cpp/container/vector" kindref="compound" external="/home/twhuang/Code/taskflow/doxygen/cppreference-doxygen-web.tag.xml">std::vector&lt;float&gt;</ref><sp/>hB<sp/>=<sp/>{<sp/><sp/><sp/><sp/><sp/><sp/></highlight><highlight class="comment">//<sp/>K<sp/>x<sp/>N<sp/>matrix</highlight><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/>11,<sp/>12,<sp/>13,<sp/>14,</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/>15,<sp/>16,<sp/>17,<sp/>18,</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/>19,<sp/>20,<sp/>21,<sp/>22</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/>};</highlight></codeline>
<codeline><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/></highlight><highlight class="keyword">const</highlight><highlight class="normal"><sp/><ref refid="cpp/container/vector" kindref="compound" external="/home/twhuang/Code/taskflow/doxygen/cppreference-doxygen-web.tag.xml">std::vector&lt;float&gt;</ref><sp/>golden<sp/>=<sp/>{<sp/><sp/></highlight><highlight class="comment">//<sp/>M<sp/>x<sp/>N<sp/>matrix</highlight><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/>548,<sp/>584,<sp/>620,<sp/>656,</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/>683,<sp/>728,<sp/>773,<sp/>818<sp/></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/>};</highlight></codeline>
<codeline><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><ref refid="cpp/container/vector" kindref="compound" external="/home/twhuang/Code/taskflow/doxygen/cppreference-doxygen-web.tag.xml">std::vector&lt;float&gt;</ref><sp/>hC(M*N);</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordtype">float</highlight><highlight class="normal"><sp/>*dA,<sp/>*dB,<sp/>*dC,<sp/>*dAlpha,<sp/>*dBeta;</highlight></codeline>
<codeline><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><ref refid="classtf_1_1Taskflow" kindref="compound">tf::Taskflow</ref><sp/>taskflow(</highlight><highlight class="stringliteral">&quot;Matrix<sp/>Multiplication&quot;</highlight><highlight class="normal">);</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><ref refid="classtf_1_1Executor" kindref="compound">tf::Executor</ref><sp/>executor;</highlight></codeline>
<codeline><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><ref refid="classtf_1_1Task" kindref="compound">tf::Task</ref><sp/>malloc_dA<sp/>=<sp/>taskflow.<ref refid="classtf_1_1FlowBuilder_1a60d7a666cab71ecfa3010b2efb0d6b57" kindref="member">emplace</ref>(</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/>[&amp;](){<sp/>dA<sp/>=<sp/>tf::cuda_malloc_device&lt;float&gt;(hA.size());<sp/>}</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/>).name(</highlight><highlight class="stringliteral">&quot;malloc_dA&quot;</highlight><highlight class="normal">);<sp/><sp/><sp/><sp/><sp/><sp/></highlight><highlight class="comment">//<sp/>allocate<sp/>GPU<sp/>memory<sp/>for<sp/>dA</highlight><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><ref refid="classtf_1_1Task" kindref="compound">tf::Task</ref><sp/>malloc_dB<sp/>=<sp/>taskflow.<ref refid="classtf_1_1FlowBuilder_1a60d7a666cab71ecfa3010b2efb0d6b57" kindref="member">emplace</ref>(</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/>[&amp;](){<sp/>dB<sp/>=<sp/>tf::cuda_malloc_device&lt;float&gt;(hB.size());<sp/>}</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/>).name(</highlight><highlight class="stringliteral">&quot;malloc_dB&quot;</highlight><highlight class="normal">);<sp/><sp/><sp/><sp/><sp/><sp/></highlight><highlight class="comment">//<sp/>allocate<sp/>GPU<sp/>memory<sp/>for<sp/>dB</highlight><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><ref refid="classtf_1_1Task" kindref="compound">tf::Task</ref><sp/>malloc_dC<sp/>=<sp/>taskflow.<ref refid="classtf_1_1FlowBuilder_1a60d7a666cab71ecfa3010b2efb0d6b57" kindref="member">emplace</ref>(</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/>[&amp;](){<sp/>dC<sp/>=<sp/>tf::cuda_malloc_device&lt;float&gt;(hC.size());<sp/>}</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/>).name(</highlight><highlight class="stringliteral">&quot;malloc_dC&quot;</highlight><highlight class="normal">);<sp/><sp/><sp/><sp/><sp/><sp/></highlight><highlight class="comment">//<sp/>allocate<sp/>GPU<sp/>memory<sp/>for<sp/>dC</highlight><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><ref refid="classtf_1_1Task" kindref="compound">tf::Task</ref><sp/>malloc_dAlpha<sp/>=<sp/>taskflow.<ref refid="classtf_1_1FlowBuilder_1a60d7a666cab71ecfa3010b2efb0d6b57" kindref="member">emplace</ref>(</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/>[&amp;](){<sp/>dAlpha<sp/>=<sp/>tf::cuda_malloc_device&lt;float&gt;(1);<sp/>}</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/>).name(</highlight><highlight class="stringliteral">&quot;malloc_dAlpha&quot;</highlight><highlight class="normal">);<sp/><sp/></highlight><highlight class="comment">//<sp/>allocate<sp/>GPU<sp/>memory<sp/>for<sp/>scalar<sp/>alpha</highlight><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><ref refid="classtf_1_1Task" kindref="compound">tf::Task</ref><sp/>malloc_dBeta<sp/>=<sp/>taskflow.<ref refid="classtf_1_1FlowBuilder_1a60d7a666cab71ecfa3010b2efb0d6b57" kindref="member">emplace</ref>(</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/>[&amp;](){<sp/>dBeta<sp/>=<sp/>tf::cuda_malloc_device&lt;float&gt;(1);<sp/>}</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/>).name(</highlight><highlight class="stringliteral">&quot;malloc_dBeta&quot;</highlight><highlight class="normal">);<sp/><sp/><sp/></highlight><highlight class="comment">//<sp/>allocate<sp/>GPU<sp/>memory<sp/>for<sp/>scalar<sp/>beta</highlight><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><ref refid="classtf_1_1Task" kindref="compound">tf::Task</ref><sp/>cublasFlow<sp/>=<sp/>taskflow.<ref refid="classtf_1_1FlowBuilder_1a60d7a666cab71ecfa3010b2efb0d6b57" kindref="member">emplace</ref>([&amp;](<ref refid="classtf_1_1cudaFlowCapturer" kindref="compound">tf::cudaFlowCapturer</ref>&amp;<sp/>capturer)<sp/>{</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/><ref refid="classtf_1_1cudaTask" kindref="compound">tf::cudaTask</ref><sp/>blas<sp/><sp/>=<sp/>capturer.make_capturer&lt;tf::cublasFlowCapturer&gt;();</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/><ref refid="classtf_1_1cudaTask" kindref="compound">tf::cudaTask</ref><sp/>alpha<sp/>=<sp/>blas-&gt;single_task([=]<sp/>__device__<sp/>()<sp/>{<sp/>*dAlpha<sp/>=<sp/>1;<sp/>})</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>.name(</highlight><highlight class="stringliteral">&quot;alpha=1&quot;</highlight><highlight class="normal">);</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/><ref refid="classtf_1_1cudaTask" kindref="compound">tf::cudaTask</ref><sp/>beta<sp/><sp/>=<sp/>blas-&gt;single_task([=]<sp/>__device__<sp/>()<sp/>{<sp/>*dBeta<sp/><sp/>=<sp/>0;<sp/>})</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>.name(</highlight><highlight class="stringliteral">&quot;beta=0&quot;</highlight><highlight class="normal">);</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/><ref refid="classtf_1_1cudaTask" kindref="compound">tf::cudaTask</ref><sp/>copyA<sp/>=<sp/>blas-&gt;copy(dA,<sp/>hA.data(),<sp/>hA.size()).name(</highlight><highlight class="stringliteral">&quot;copyA&quot;</highlight><highlight class="normal">);<sp/></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/><ref refid="classtf_1_1cudaTask" kindref="compound">tf::cudaTask</ref><sp/>copyB<sp/>=<sp/>blas-&gt;copy(dB,<sp/>hB.data(),<sp/>hB.size()).name(</highlight><highlight class="stringliteral">&quot;copyB&quot;</highlight><highlight class="normal">);</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/><ref refid="classtf_1_1cudaTask" kindref="compound">tf::cudaTask</ref><sp/>gemm<sp/><sp/>=<sp/>blas-&gt;c_gemm(CUBLAS_OP_N,<sp/>CUBLAS_OP_N,</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/>M,<sp/>N,<sp/>K,<sp/>dAlpha,<sp/>dA,<sp/>K,<sp/>dB,<sp/>N,<sp/>dBeta,<sp/>dC,<sp/>N</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/>).<ref refid="classtf_1_1cudaTask_1ab81b4f71a44af8d61758524f0c274962" kindref="member">name</ref>(</highlight><highlight class="stringliteral">&quot;C<sp/>=<sp/>alpha<sp/>*<sp/>A<sp/>*<sp/>B<sp/>+<sp/>beta<sp/>*<sp/>C&quot;</highlight><highlight class="normal">);</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/><ref refid="classtf_1_1cudaTask" kindref="compound">tf::cudaTask</ref><sp/>copyC<sp/>=<sp/>blas-&gt;copy(hC.data(),<sp/>dC,<sp/>hC.size()).name(</highlight><highlight class="stringliteral">&quot;copyC&quot;</highlight><highlight class="normal">);</highlight></codeline>
<codeline><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/>gemm.<ref refid="classtf_1_1cudaTask_1a4a9ca1a34bac47e4c9b04eb4fb2f7775" kindref="member">succeed</ref>(alpha,<sp/>beta,<sp/>copyA,<sp/>copyB)</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>.<ref refid="classtf_1_1cudaTask_1abdd68287ec4dff4216af34d1db44d1b4" kindref="member">precede</ref>(copyC);</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/>}).name(</highlight><highlight class="stringliteral">&quot;cublasFlow&quot;</highlight><highlight class="normal">);</highlight></codeline>
<codeline><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/>cublasFlow.<ref refid="classtf_1_1Task_1a331b1b726555072e7c7d10941257f664" kindref="member">succeed</ref>(<sp/><sp/></highlight><highlight class="comment">//<sp/>cublasFlow<sp/>runs<sp/>after<sp/>GPU<sp/>memory<sp/>operations</highlight><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/>malloc_dA,<sp/>malloc_dB,<sp/>malloc_dC,<sp/>malloc_dAlpha,<sp/>malloc_dBeta</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/>);</highlight></codeline>
<codeline><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/>executor.<ref refid="classtf_1_1Executor_1a519777f5783981d534e9e53b99712069" kindref="member">run</ref>(taskflow).wait();</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><ref refid="cpp/io/basic_ostream" kindref="compound" external="/home/twhuang/Code/taskflow/doxygen/cppreference-doxygen-web.tag.xml">std::cout</ref><sp/>&lt;&lt;<sp/></highlight><highlight class="stringliteral">&quot;Matrix<sp/>C:\n&quot;</highlight><highlight class="normal">;</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordflow">for</highlight><highlight class="normal">(</highlight><highlight class="keywordtype">int</highlight><highlight class="normal"><sp/>m=0;<sp/>m&lt;M;<sp/>m++)<sp/>{</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keywordflow">for</highlight><highlight class="normal">(</highlight><highlight class="keywordtype">int</highlight><highlight class="normal"><sp/>n=0;<sp/>n&lt;N;<sp/>n++)<sp/>{</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><ref refid="cpp/io/basic_ostream" kindref="compound" external="/home/twhuang/Code/taskflow/doxygen/cppreference-doxygen-web.tag.xml">std::cout</ref><sp/>&lt;&lt;<sp/>hC[m*N+n]<sp/>&lt;&lt;<sp/></highlight><highlight class="charliteral">&apos;<sp/>&apos;</highlight><highlight class="normal">;</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/>}</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/><ref refid="cpp/io/basic_ostream" kindref="compound" external="/home/twhuang/Code/taskflow/doxygen/cppreference-doxygen-web.tag.xml">std::cout</ref><sp/>&lt;&lt;<sp/></highlight><highlight class="charliteral">&apos;\n&apos;</highlight><highlight class="normal">;</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/>}</highlight></codeline>
<codeline><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/>0;</highlight></codeline>
<codeline><highlight class="normal">}</highlight></codeline>
</programlisting></para><para>The program uses five static tasks to allocate GPU memory for <computeroutput>dA</computeroutput>, <computeroutput>dB</computeroutput>, <computeroutput>dC</computeroutput>, <computeroutput>dAlpha</computeroutput>, and <computeroutput>dBeta</computeroutput>, in parallel such that the expensive GPU memory operations can overlap with each other as much as possible. The <computeroutput>cublasFlow</computeroutput> task spawns a cublasFlow capturer of (1) one single kernel task <computeroutput>alpha</computeroutput> to set <computeroutput>dAlpha</computeroutput> to 1, (2) one single kernel task <computeroutput>beta</computeroutput> to set <computeroutput>dBeta</computeroutput> to 0, (3) two copy tasks, <computeroutput>copyA</computeroutput> and <computeroutput>copyB</computeroutput>, to copy data from CPU to GPU, (4) one kernel task <computeroutput>gemm</computeroutput> to perform <computeroutput>dC = dA * dB</computeroutput>, and (5) one copy task <computeroutput>copyC</computeroutput> to copy the result from GPU to CPU.</para><para><dotfile name="/home/twhuang/Code/taskflow/doxygen/images/cublasflow_capturer_gemm.dot"></dotfile>
</para></sect2>
</sect1>
<sect1 id="LinearAlgebracublasFlowCapturer_1cublasFlowCapturerExtension">
<title>Include Other cuBLAS Methods</title>
<para>We do not include all the cuBLAS functions but users can easily make extension. tf::cublasFlowCapturer is derived from tf::cudaFlowCapturerBase and is created from a factory interface tf::cudaFlowCapturer::make_capturer. Each tf::cudaFlowCapturerBase object has a pointer accessible by tf::cudaFlowCapturerBase::factory in which you can use <ref refid="classtf_1_1cudaFlowCapturer_1ad0d937ae0d77239f148b66a77e35db41" kindref="member">tf::cudaFlowCapturer::on</ref> together with tf::cublasFlowCapturer::native_handle to capture other cuBLAS functions that are currently not available in <ref refid="classtf_1_1cudaFlowCapturer" kindref="compound">tf::cudaFlowCapturer</ref>. The following example captures the Hermitian rank-k update using <computeroutput>cublasCherkx</computeroutput>.</para><para><programlisting filename=".cpp"><codeline><highlight class="normal">taskflow.<ref refid="classtf_1_1FlowBuilder_1a60d7a666cab71ecfa3010b2efb0d6b57" kindref="member">emplace</ref>([&amp;](<ref refid="classtf_1_1cudaFlowCapturer" kindref="compound">tf::cudaFlowCapturer</ref>&amp;<sp/>capturer){</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/></highlight><highlight class="comment">//<sp/>create<sp/>a<sp/>cublasFlow<sp/>capturer</highlight><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/></highlight><highlight class="keyword">auto</highlight><highlight class="normal"><sp/>blas<sp/>=<sp/>capturer.make_capturer&lt;tf::cublasFlowCapturer&gt;();</highlight></codeline>
<codeline><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/></highlight><highlight class="comment">//<sp/>use<sp/>the<sp/>base<sp/>method<sp/>tf::cudaFlowCapturer::on<sp/>to<sp/>capture<sp/>other<sp/>functions</highlight><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/>blas-&gt;factory()-&gt;on([&amp;](cudaStream_t<sp/>stream){</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/>cublasSetStream(blas-&gt;native_handle(),<sp/>stream);</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/>cublasCherkx(blas-&gt;native_handle(),<sp/>your_args...);</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/>}).name(</highlight><highlight class="stringliteral">&quot;Hermitian<sp/>rank-k<sp/>update&quot;</highlight><highlight class="normal">);</highlight></codeline>
<codeline><highlight class="normal"></highlight></codeline>
<codeline><highlight class="normal">}).name(</highlight><highlight class="stringliteral">&quot;capturer&quot;</highlight><highlight class="normal">);</highlight></codeline>
</programlisting></para><para><simplesect kind="warning"><para>While tf::cublasFlowCapturer::native_handle returns the native cublas handle, you must not change its properties.</para></simplesect>
By default, we associate the native cublas handler with <computeroutput>CUBLAS_POINTER_MODE_DEVICE</computeroutput>. The two scalars, <computeroutput>alpha</computeroutput> and/or <computeroutput>beta</computeroutput>, and input/output matrices must be accessible in GPU memory space.</para></sect1>
<sect1 id="LinearAlgebracublasFlowCapturer_1cublasFlowCapturerKnowMore">
<title>Know More About cublasFlow Capturer</title>
<para>We summarize below resources for you to know more about tf::cublasFlowCapturer:<itemizedlist>
<listitem><para>Study the reference of tf::cublasFlowCapturer and <ref refid="classtf_1_1cudaFlowCapturer" kindref="compound">tf::cudaFlowCapturer</ref></para></listitem><listitem><para>Contribute to cublas_flow.hpp by adding more BLAS methods</para></listitem><listitem><para>See the complete list of BLAS functions offered by <ulink url="https://docs.nvidia.com/cuda/cublas/index.html">cuBLAS</ulink> </para></listitem></itemizedlist>
</para></sect1>
    </detaileddescription>
  </compounddef>
</doxygen>
